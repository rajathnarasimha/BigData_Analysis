# -*- coding: utf-8 -*-
"""ETLChallenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mICjVKp2u63xUrL5I_xNJAWFAivwpAe_
"""

# Install Java, Spark, and Findspark
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz
!tar xf spark-2.4.5-bin-hadoop2.7.tgz
!pip install -q findspark
# Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.5-bin-hadoop2.7"
# Start a SparkSession
import findspark
findspark.init()

!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ETLChallenge").config("spark.driver.extraClassPath","/content/postgresql-42.2.9.jar").getOrCreate()

#Extract
# Read in data from S3 Buckets
from pyspark import SparkFiles
url ="https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz"
spark.sparkContext.addFile(url)
reveiw_data_df = spark.read.csv(SparkFiles.get("amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz"), sep="\t", header=True)
reveiw_data_df.show(5)

# Count the number of records (rows) in the dataset.
reveiw_data_df.count()

reveiw_data_df.dtypes

# Transform
#Drop null values from the table
dropna_df=reveiw_data_df.dropna()

dropna_df.count()

#Drop duplicate values

df_dedupe = dropna_df.dropDuplicates(['product_id'])
df_reverse = dropna_df.sort((["product_id"]), ascending= False)
df_dedupe.join(df_reverse,['product_id'],'inner')
#Check for duplicates if any: df_dedupe.groupby(['product_id']).count().where('count > 1').sort('count', ascending=False).show()
df_dedupe.count()

#Drop duplicate values
df_dedupe1 = df_dedupe.dropDuplicates(['customer_id'])
df_reverse1 = df_dedupe.sort((["customer_id"]), ascending= False)
df_dedupe1.join(df_reverse1,['customer_id'],'inner')
#Check for duplicates if any: df_dedupe1.groupby(['customer_id']).count().where('count > 1').sort('count', ascending=False).show()
df_dedupe1.count()

#Clean the datatypes according to the SQL schema
cleaned_df = df_dedupe1.withColumn("customer_id",df_dedupe1["customer_id"].cast('int'))
cleaned_df = cleaned_df.withColumn("product_parent",cleaned_df["product_parent"].cast('int'))
cleaned_df = cleaned_df.withColumn("review_date",cleaned_df["review_date"].cast('date'))
cleaned_df = cleaned_df.withColumn("star_rating",cleaned_df["star_rating"].cast('int'))
cleaned_df = cleaned_df.withColumn("helpful_votes",cleaned_df["helpful_votes"].cast('int'))
cleaned_df = cleaned_df.withColumn("total_votes",cleaned_df["total_votes"].cast('int'))
cleaned_df.dtypes

cleaned_df.count()

#Customer_count in the SQL schema is not present in the input tsv file hence generating a new column with the same name and assiging row number.
from pyspark.sql.window import Window as W
from pyspark.sql import functions as F
cleaned_df = cleaned_df.withColumn("customer_count", F.monotonically_increasing_id())
windowSpec = W.orderBy("customer_count")
cleaned_df.withColumn("customer_count", F.row_number().over(windowSpec)).show()

cleaned_df.count()

cleaned_df.dtypes

cleaned_review_df = cleaned_df.select(["review_id", "customer_id", "product_id","product_parent", "review_date"])
cleaned_review_df.show(5)

cleaned_products_df = cleaned_df.select(["product_id", "product_title"])
cleaned_products_df.show(5)

cleaned_customers_df = cleaned_df.select(["customer_id", "customer_count"])
cleaned_customers_df.show(5)

vine_table_df = cleaned_df.select(["review_id", "star_rating", "helpful_votes","total_votes", "vine"])
vine_table_df.show(5)

# Load
# Configure settings for RDS
mode = "append"
jdbc_url="jdbc:postgresql://database-1.coxa50m0widb.us-east-2.rds.amazonaws.com:5432/challenge_db"
config = {"user":"postgres",
          "password": "Asdf1234*",
          "driver":"org.postgresql.Driver"}

# Write DataFrame to review_id_table in RDS
cleaned_review_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)

# Write DataFrame to products table in RDS
cleaned_products_df.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=config)

# Write DataFrame to customers table in RDS
cleaned_customers_df.write.jdbc(url=jdbc_url, table='customers', mode=mode, properties=config)

# Write DataFrame to vine_table in RDS
vine_table_df.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)